{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93fa12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "\n",
    "import conntility\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b5cc9b",
   "metadata": {},
   "source": [
    "## Part 1:\n",
    "Measure the strengths of statistical interactions in connectivity based on individual morphologies that are not capture by a fourth-order simplifed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f80eb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mtype\n",
       "L2_PTPC    9206\n",
       "L3_PTPC    6440\n",
       "L3_STPC    3950\n",
       "L3_PC       468\n",
       "IN            0\n",
       "BC            0\n",
       "L4_PC         0\n",
       "L5_PC         0\n",
       "L6_PC         0\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_rat = \"/gpfs/bbp.cscs.ch/project/proj159/home/barros/conn_matrix/Rat_623um_squared_struc_conmat_filtered_compressed.h5\"\n",
    "fn_human = \"/gpfs/bbp.cscs.ch/project/proj159/home/barros/conn_matrix/Human_960um_squared_struc_conmat_filtered_compressed.h5\"\n",
    "fn_human_func = \"/gpfs/bbp.cscs.ch/project/proj159/home/barros/conn_matrix/Human_NEWfunct_conmat_filtered_compressed.h5\"\n",
    "\n",
    "# https://doi.org/10.5281/zenodo.13849415\n",
    "fn_microns = \"/gpfs/bbp.cscs.ch/project/proj159/home/reimann/microns/microns_mm3_connectome_v1181.h5\"\n",
    "fn_control = \"/gpfs/bbp.cscs.ch/project/proj159/home/reimann/third_order_mdl_instances_human.h5\"\n",
    "\n",
    "loaded = \"human_functional\"  # Set to human to analyze that instead\n",
    "\n",
    "if loaded == \"rat\":\n",
    "    M_h = conntility.ConnectivityMatrix.from_h5(fn_rat)\n",
    "    col_y = \"depth\"\n",
    "    col_xz = [\"ss_flat_x\", \"ss_flat_y\"]\n",
    "    bin_sz = 50.0\n",
    "elif loaded == \"human\":\n",
    "    M_h = conntility.ConnectivityMatrix.from_h5(fn_human)\n",
    "    col_y = \"y\"\n",
    "    col_xz = [\"x\", \"z\"]\n",
    "    bin_sz = 50.0 * 1.76\n",
    "elif loaded == \"human_functional\":\n",
    "    M_h = conntility.ConnectivityMatrix.from_h5(fn_human_func)\n",
    "    col_y = \"y\"\n",
    "    col_xz = [\"x\", \"z\"]\n",
    "    bin_sz = 50.0 * 1.76\n",
    "elif loaded == \"microns\":\n",
    "    M_h = conntility.ConnectivityMatrix.from_h5(fn_microns, \"condensed\")\n",
    "    M_h = M_h.index(\"cell_type\").isin([\"L2a\", \"L2b\", \"L2c\", \"L3a\", \"L3b\"])\n",
    "    for col in [\"x\", \"y\", \"z\"]:\n",
    "        M_h.add_vertex_property(col, M_h.vertices[col + \"_nm\"].values / 1000)\n",
    "    M_h.add_vertex_property(\"mtype\", M_h.vertices[\"cell_type\"].values)\n",
    "    col_y = \"y\"\n",
    "    col_xz = [\"x\", \"z\"]\n",
    "    bin_sz = 50.0\n",
    "elif loaded == \"control\":\n",
    "    M_h = conntility.ConnectivityMatrix.from_h5(fn_control, \"instance1\")\n",
    "    col_y = \"y\"\n",
    "    col_xz = [\"x\", \"z\"]\n",
    "    bin_sz = 50.0 * 1.76\n",
    "    \n",
    "def make_spatial_bins(M_h, cols, bin_sz):\n",
    "    _data = M_h.vertices[cols]\n",
    "    delta = _data.max() - _data.min()\n",
    "\n",
    "    sz = numpy.sqrt((delta.values ** 2).sum())\n",
    "    if len(delta) == 1: # case 1d: negative and positive bins\n",
    "        bins = numpy.arange(0, (bin_sz * numpy.ceil(sz / bin_sz)) + bin_sz, bin_sz)\n",
    "        bins = numpy.hstack([-bins[:0:-1], bins])\n",
    "    else: # case 2d: Only positive bins, but exclude 0 dist\n",
    "        bins = numpy.arange(0, (bin_sz * numpy.ceil(sz / bin_sz)) + bin_sz, bin_sz)\n",
    "        bins = numpy.hstack([[0, 1E-12], bins[1:]])\n",
    "    return bins\n",
    "\n",
    "dbins_xz = make_spatial_bins(M_h, col_xz, bin_sz)\n",
    "binid_xz = numpy.arange(0, len(dbins_xz) + 1)\n",
    "\n",
    "dbins_y = make_spatial_bins(M_h, [col_y], bin_sz)\n",
    "binid_y = numpy.arange(0, len(dbins_y) + 1)\n",
    "\n",
    "bin_centers_y = 0.5 * (dbins_y[:-1] + dbins_y[1:])\n",
    "bin_centers_xz = 0.5 * (dbins_xz[1:-1] + dbins_xz[2:])\n",
    "\n",
    "M_h.vertices[\"mtype\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "141cea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import KDTree\n",
    "\n",
    "_coords = col_xz + [col_y]\n",
    "tree = KDTree(M_h.vertices[_coords].values)\n",
    "\n",
    "_, nn_id = tree.query(M_h.vertices[_coords], k=2)\n",
    "nn_id = nn_id[:, 1]  # nn_id[:, 0] is the original node, which has distance 0. nn_id[:, 1] is neighbor\n",
    "\n",
    "# Lookup from pre / post ids to edge ids\n",
    "edge_id_lookup = M_h._edge_indices.reset_index(drop=True).reset_index().set_index([\"row\", \"col\"])[\"index\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc6eafc",
   "metadata": {},
   "source": [
    "For this analysis we build a table as follows:\n",
    "\n",
    "For each possible xz/y bin and value of n and m:\n",
    "    We count the number of instances where a neuron N_a at offset xz, y from another neuron N_b has n touches/synapses with N_b, and the nearest neighbor of N_a has m touches/synapses with N_b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28739ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_pre_chunk(chunk_pre):\n",
    "    # Which offset bin the pairs fall into\n",
    "    Dxz = cdist(M_h.vertices.iloc[chunk_pre][col_xz], M_h.vertices[col_xz]) # PRE X POST\n",
    "    Dxz = numpy.digitize(Dxz, dbins_xz) - 2  # -2 means distance = 0 will be bin id -1. That is the one to exclude.\n",
    "\n",
    "    Dy = M_h.vertices.iloc[chunk_pre][[col_y]].values - M_h.vertices[[col_y]].values.transpose() # PRE X POST\n",
    "    Dy = numpy.digitize(Dy, dbins_y) - 1  # NOTE: Negative values -> upwards connection\n",
    "    \n",
    "    # Numer of touches between them\n",
    "    j, i = numpy.meshgrid(numpy.arange(len(M_h)), chunk_pre) # Chunk is i.: Presyn.\n",
    "    assert j.shape == Dy.shape\n",
    "    con_index = pandas.MultiIndex.from_frame(pandas.DataFrame({\"row\": i.flatten(), \"col\": j.flatten()}))\n",
    "    edge_ids = edge_id_lookup.reindex(con_index, fill_value=-1).values\n",
    "    \n",
    "    touch_count = numpy.zeros(len(edge_ids))\n",
    "    v = edge_ids > 0\n",
    "    touch_count[v] = M_h.edges[\"count\"].values[edge_ids[v]]\n",
    "    \n",
    "    # Number of touches with nearest neighbor of the presynaptic neuron\n",
    "    _pre = nn_id[i.flatten()]; _post = j.flatten()\n",
    "    con_index = pandas.MultiIndex.from_frame(pandas.DataFrame({\"row\": _pre, \"col\": _post}))\n",
    "    edge_ids = edge_id_lookup.reindex(con_index, fill_value=-1).values\n",
    "    \n",
    "    touch_count_nnpre = numpy.zeros(len(edge_ids))\n",
    "    v = edge_ids > 0\n",
    "    touch_count_nnpre[v] = M_h.edges[\"count\"].values[edge_ids[v]]\n",
    "    collision_pre = _pre != _post\n",
    "    \n",
    "    # Number of touches with nearest neighbor of the postsynaptic neuron\n",
    "    _pre = i.flatten(); _post = nn_id[j.flatten()]\n",
    "    con_index = pandas.MultiIndex.from_frame(pandas.DataFrame({\"row\": _pre, \"col\": _post}))\n",
    "    edge_ids = edge_id_lookup.reindex(con_index, fill_value=-1).values\n",
    "    \n",
    "    touch_count_nnpost = numpy.zeros(len(edge_ids))\n",
    "    v = edge_ids > 0\n",
    "    touch_count_nnpost[v] = M_h.edges[\"count\"].values[edge_ids[v]]\n",
    "    collision_post = _pre != _post\n",
    "    \n",
    "    # Count instances of each\n",
    "    ret_pre = pandas.DataFrame({\n",
    "        \"xz\": Dxz.flatten()[collision_pre],\n",
    "        \"y\": Dy.flatten()[collision_pre],\n",
    "        \"touches_pair\": touch_count[collision_pre],\n",
    "        \"touches_nn_pre\": touch_count_nnpre[collision_pre],\n",
    "    }).value_counts()\n",
    "    \n",
    "    ret_post = pandas.DataFrame({\n",
    "        \"xz\": Dxz.flatten()[collision_post],\n",
    "        \"y\": Dy.flatten()[collision_post],\n",
    "        \"touches_pair\": touch_count[collision_post],\n",
    "        \"touches_nn_post\": touch_count_nnpost[collision_post],\n",
    "    }).value_counts()\n",
    "    return ret_pre, ret_post\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb78badd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [03:50<01:17, 15.41s/it]"
     ]
    }
   ],
   "source": [
    "#In each offset-bin: How many pairs exist?\n",
    "full_master_pre = []\n",
    "full_master_post = []\n",
    "chunk_sz = 1000\n",
    "chunking = numpy.arange(0, len(M_h) + chunk_sz, chunk_sz)\n",
    "\n",
    "chunk_pre = numpy.arange(chunking[0], numpy.minimum(chunking[1], len(M_h)))\n",
    "full_master_pre, full_master_post = for_pre_chunk(chunk_pre)\n",
    "\n",
    "for a, b in tqdm.tqdm(list(zip(chunking[1:-1], chunking[2:]))):\n",
    "    chunk_pre = numpy.arange(a, numpy.minimum(b, len(M_h)))\n",
    "    new_master_pre, new_master_post = for_pre_chunk(chunk_pre)\n",
    "    full_master_pre = full_master_pre.add(new_master_pre, fill_value=0)\n",
    "    full_master_post = full_master_post.add(new_master_post, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d0bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_master_pre = full_master_pre.drop(-1, axis=0).reset_index()\n",
    "full_master_post = full_master_post.drop(-1, axis=0).reset_index()\n",
    "\n",
    "\n",
    "assert (full_master_pre[[\"xz\", \"y\"]] >= 0).all().all()\n",
    "assert (full_master_post[[\"xz\", \"y\"]] >= 0).all().all()\n",
    "\n",
    "assert (full_master_pre[\"xz\"] < len(binid_xz)).all()\n",
    "assert (full_master_pre[\"y\"] < len(binid_y)).all()\n",
    "assert (full_master_post[\"xz\"] < len(binid_xz)).all()\n",
    "assert (full_master_post[\"y\"] < len(binid_y)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aa61aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_master_pre[\"xz\"] = bin_centers_xz[full_master_pre[\"xz\"]]\n",
    "full_master_pre[\"y\"] = bin_centers_y[full_master_pre[\"y\"]]\n",
    "\n",
    "full_master_post[\"xz\"] = bin_centers_xz[full_master_post[\"xz\"]]\n",
    "full_master_post[\"y\"] = bin_centers_y[full_master_post[\"y\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d16da",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fn = \"/gpfs/bbp.cscs.ch/project/proj159/home/reimann/connectivity_higher_order_effect_24-11-11.h5\"\n",
    "\n",
    "full_master_pre.to_hdf(out_fn, key=\"{0}/pre\".format(loaded))\n",
    "full_master_post.to_hdf(out_fn, key=\"{0}/post\".format(loaded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a6e852",
   "metadata": {},
   "source": [
    "## Part 2:\n",
    "The above characterizes statistical interactions within one spatial bin. Here, we consider interactions _between_ bins. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e08e0f",
   "metadata": {},
   "source": [
    "For this analysis we build a table as follows:\n",
    "\n",
    "For each possible xz/y bin, value of n neuron N_b:\n",
    "    We count the number of instances where a neuron N_a at offset xz, y from N_b has n touches/synapses with N_b. The difference  with respect to the previous analysis is that we count separately for each neuron N_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49e4dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_id_lookup = M_h._edge_indices.reset_index(drop=True).reset_index().set_index([\"row\", \"col\"])[\"index\"]\n",
    "\n",
    "def for_chunk(chunk, interaction_for=\"pre\"):\n",
    "    # Which offset bin the pairs fall into\n",
    "    if interaction_for == \"pre\": # Pre means: from chunk to all neurons\n",
    "        Dxz = cdist(M_h.vertices.iloc[chunk][col_xz], M_h.vertices[col_xz]) # PRE X POST\n",
    "        Dxz = numpy.digitize(Dxz, dbins_xz) - 2  # -2 means distance = 0 will be bin id -1\n",
    "\n",
    "        Dy = M_h.vertices.iloc[chunk][[col_y]].values - M_h.vertices[[col_y]].values.transpose() # PRE X POST\n",
    "        Dy = numpy.digitize(Dy, dbins_y) - 1  # NOTE: Negative values -> upwards connection\n",
    "        \n",
    "        j, i = numpy.meshgrid(numpy.arange(len(M_h)), chunk) # chunk is i: presyn. => consider outgoing\n",
    "        node_id = i.flatten()\n",
    "    elif interaction_for == \"post\":\n",
    "        Dxz = cdist(M_h.vertices[col_xz], M_h.vertices.iloc[chunk][col_xz]) # PRE X POST\n",
    "        Dxz = numpy.digitize(Dxz, dbins_xz) - 2  # -2 means distance = 0 will be bin id -1\n",
    "\n",
    "        Dy = M_h.vertices[[col_y]].values - M_h.vertices.iloc[chunk][[col_y]].values.transpose() # PRE X POST\n",
    "        Dy = numpy.digitize(Dy, dbins_y) - 1  # NOTE: Negative values -> upwards connection\n",
    "        \n",
    "        j, i = numpy.meshgrid(chunk, numpy.arange(len(M_h))) # chunk is j: postsyn. => consider incoming\n",
    "        node_id = j.flatten()\n",
    "    \n",
    "    # Numer of touches between them\n",
    "    assert j.shape == Dy.shape\n",
    "    con_index = pandas.MultiIndex.from_frame(pandas.DataFrame({\"row\": i.flatten(), \"col\": j.flatten()}))\n",
    "    edge_ids = edge_id_lookup.reindex(con_index, fill_value=-1).values\n",
    "    \n",
    "    touch_count = numpy.zeros(len(edge_ids))\n",
    "    v = edge_ids > 0\n",
    "    touch_count[v] = M_h.edges[\"count\"].values[edge_ids[v]]\n",
    "    touch_count.reshape(Dy.shape)\n",
    "        \n",
    "    # Count instances of each\n",
    "    ret = pandas.DataFrame({\n",
    "        \"xz\": Dxz.flatten(),\n",
    "        \"y\": Dy.flatten(),\n",
    "        \"touches_pair\": touch_count,\n",
    "        \"node_id\": node_id,\n",
    "    }).value_counts()\n",
    "    ret.name = \"count\"\n",
    "    return ret\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e1fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_pre = []\n",
    "df_for_post = []\n",
    "\n",
    "chunk_sz = 2000\n",
    "chunking = numpy.arange(0, len(M_h) + chunk_sz, chunk_sz)\n",
    "\n",
    "for a, b in tqdm.tqdm(list(zip(chunking[:-1], chunking[1:]))):\n",
    "    chunk = numpy.arange(a, numpy.minimum(b, len(M_h)))\n",
    "    df_for_pre.append(for_chunk(chunk, interaction_for=\"pre\"))\n",
    "    df_for_post.append(for_chunk(chunk, interaction_for=\"post\"))\n",
    "    \n",
    "\n",
    "df_for_pre = pandas.concat(df_for_pre, axis=0).drop(-1, axis=0, level=\"xz\")\n",
    "df_for_post = pandas.concat(df_for_post, axis=0).drop(-1, axis=0, level=\"xz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330dfa7d",
   "metadata": {},
   "source": [
    "Next, we convert the information in the table as follows: For each offset bin and neuron calculate the number of connected neurons in that bin and the total number of neurons (connected or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b053f391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We introduce a threshold for minimum number of touches to make the results sparser.\n",
    "# For Microns connectivity or functional connectivity set to 1.\n",
    "thresh = 1  # HOW MANY TOUCHES NEED TO BE REACHED FOR A CONNECTION\n",
    "\n",
    "def average_P(df_in):\n",
    "    return (df_in[\"N\"] * df_in[\"P\"]).sum() / df_in[\"N\"].sum()\n",
    "    \n",
    "# For each pre- or post-neuron: How many partners does it have in each offset bin (N) and what fraction is connected (P)?\n",
    "def count_stats_per_spatial_bin(df_in, add_normalized=True, more_normalized=True):\n",
    "    def count_n_and_p(df_in):\n",
    "        M = df_in.pivot(index=\"touches_pair\", columns=\"node_id\", values=\"count\") # \"node_id\"\n",
    "        V = pandas.concat([M.sum(axis=0), M.loc[thresh:].sum(axis=0) / M.sum(axis=0)], axis=1, keys=[\"N\", \"P\"])\n",
    "        return V\n",
    "    df_out = df_in.reset_index().groupby([\"xz\", \"y\"]).apply(count_n_and_p)\n",
    "    \n",
    "    df_out[\"M\"] = df_out[\"N\"] * df_out[\"P\"]\n",
    "    if add_normalized or more_normalized:\n",
    "        p_per_nrn = df_out.groupby(\"node_id\").apply(average_P)\n",
    "    if add_normalized:\n",
    "        df_out[\"E\"] = df_out[\"N\"] * p_per_nrn\n",
    "        df_out[\"dM\"] = df_out[\"M\"] - df_out[\"E\"]\n",
    "        df_out[\"dP\"] = df_out[\"P\"] - df_out[\"E\"] / df_out[\"N\"]\n",
    "    if more_normalized:\n",
    "        mean_p_profile = df_out.groupby([\"xz\", \"y\"])[\"P\"].mean()\n",
    "        p_per_nrn = p_per_nrn / p_per_nrn.mean()\n",
    "        expected_profile = mean_p_profile.values.reshape((-1, 1)) * p_per_nrn.values.reshape((1, -1))\n",
    "        expected_profile = pandas.DataFrame(expected_profile, index=mean_p_profile.index, columns=p_per_nrn.index)\n",
    "        expected_profile = expected_profile.stack()\n",
    "        df_out[\"E_spat\"] = df_out[\"N\"] * expected_profile\n",
    "        df_out[\"dM_spat\"] = df_out[\"M\"] - df_out[\"E_spat\"]\n",
    "        df_out[\"dP_spat\"] = df_out[\"P\"] - expected_profile\n",
    "    return df_out\n",
    "\n",
    "df_n_p_pre = count_stats_per_spatial_bin(df_for_pre)\n",
    "df_n_p_post = count_stats_per_spatial_bin(df_for_post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b8096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "def count_number_of_matching_pairs(df_in):\n",
    "    nn = (~numpy.isnan(df_in)).astype(int)\n",
    "    nn = sparse.csc_matrix(nn.values)\n",
    "    nncount = nn.transpose() * nn\n",
    "    nncount = pandas.DataFrame(numpy.array(nncount.todense()),\n",
    "                               index=df_in.columns, columns=df_in.columns)\n",
    "    return nncount\n",
    "\n",
    "# What is the correlation between observed values of \"P\" over pre- or post-neurons in a given offset bin?\n",
    "def observations_to_cc_of_offset_bins(df_in, mode=\"valid\", use=\"P\", thresh_pairs=100, thresh_count=10):\n",
    "    df_in = df_in[df_in[\"N\"] >= thresh_count]\n",
    "    df_cc = df_in.reset_index()\n",
    "    # columns: offset bin. index: neuron. values: observed \"P\", or NaN if the neuron has no other neuron at that offset\n",
    "    df_cc = df_cc.pivot(columns=[\"xz\", \"y\"], index=\"node_id\", values=use)\n",
    "    # How to deal with NaN values? The default behavior of pandas is to ignore them, i.e. calculate correlation only\n",
    "    # for observations where neither of a pair is NaN. \n",
    "    # A more conservative approach is to assume that the neuron would have the overall mean connection probability.\n",
    "    # Finally we can add a small, random noise to all observations. That way, bins with all zeros will show up\n",
    "    # to have zero correlation with all others. Else they will have NaN correlation\n",
    "    if mode == \"valid\":\n",
    "        ret = df_cc.corr()\n",
    "    else:\n",
    "        mn_P = df_in.groupby([\"xz\", \"y\"]).apply(lambda _x: (_x[\"N\"] * _x[use]).sum() / _x[\"N\"].sum())\n",
    "        ret = (df_cc.fillna(mn_P) + 1E-9 * numpy.random.rand(*df_cc.shape)).corr()\n",
    "    if thresh_pairs is not None:\n",
    "        nncount = count_number_of_matching_pairs(df_cc)\n",
    "        ret[nncount < thresh_pairs] = numpy.NaN\n",
    "    return ret\n",
    "    \n",
    "CC_pre = observations_to_cc_of_offset_bins(df_n_p_pre, use=\"P\")\n",
    "CC_post = observations_to_cc_of_offset_bins(df_n_p_post, use=\"P\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e707a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_index(idx):\n",
    "    idx = idx.to_frame().reset_index(drop=True)\n",
    "    idx[\"xz\"] = bin_centers_xz[idx[\"xz\"]]\n",
    "    idx[\"y\"] = bin_centers_y[idx[\"y\"]]\n",
    "    return pandas.MultiIndex.from_frame(idx)\n",
    "\n",
    "CC_post.index = translate_index(CC_post.index)\n",
    "CC_post.columns = translate_index(CC_post.columns)\n",
    "CC_pre.index = translate_index(CC_pre.index)\n",
    "CC_pre.columns = translate_index(CC_pre.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5986c0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fn = \"/gpfs/bbp.cscs.ch/project/proj159/home/reimann/connectivity_higher_order_interactions_24-11-11.h5\"\n",
    "\n",
    "CC_pre.to_hdf(out_fn, key=\"{0}/{1}/pre\".format(loaded, thresh))\n",
    "CC_post.to_hdf(out_fn, key=\"{0}/{1}/post\".format(loaded, thresh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2950ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_edge_count_pre = df_n_p_pre.groupby([\"xz\", \"y\"]).apply(lambda _x: _x[\"N\"].mean())\n",
    "mn_edge_count_pre.index = translate_index(mn_edge_count_pre.index)\n",
    "mn_edge_count_post = df_n_p_post.groupby([\"xz\", \"y\"]).apply(lambda _x: _x[\"N\"].mean())\n",
    "mn_edge_count_post.index = translate_index(mn_edge_count_post.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaddaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fn = \"/gpfs/bbp.cscs.ch/project/proj159/home/reimann/connectivity_higher_order_interactions_24-11-11.h5\"\n",
    "\n",
    "mn_edge_count_pre.to_hdf(out_fn, key=\"{0}/{1}/pre_N\".format(loaded, thresh))\n",
    "mn_edge_count_post.to_hdf(out_fn, key=\"{0}/{1}/post_N\".format(loaded, thresh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7440c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bluepysnap",
   "language": "python",
   "name": "bluepysnap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
